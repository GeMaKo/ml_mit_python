{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. Classification Walkthrough: Titanic Dataset\n",
    "Machine Learning Pocket Reference by Matt Harrison Published by O'Reilly Media, Inc., 2019 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a Question\n",
    "\n",
    "In this example, we want to create a **predictive model** to answer a question. It will classify whether an individual survives the Titanic ship catastrophe based on individual and trip characteristics. This is a toy example, but it serves as a pedagogical tool for showing many steps of modeling. Our model should be able to take passenger information and predict whether that passenger would survive on the Titanic.\n",
    "\n",
    "This is a **classification** question, as we are predicting a label for **survival**; either they survived or they died. So we have a **binary classification** task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms for Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We typically train a model with a **matrix of data**. (I prefer to use pandas `DataFrames` because it is very nice to have column labels, but numpy arrays work as well.)\n",
    "\n",
    "For **supervised learning**, such as **regression or classification**, our intent is to have a fuction that transforms features into a label. If we were to write this as an algebra formula, it would look like this:\n",
    "$$y = f(\\textbf{X})$$\n",
    "\n",
    "$X$ is a matrix. Each **row represents a sample of data** or information about an individual. Every **column in X is a feature**. The **output** of our function, y, is a vector that contains **labels** (for classification) or **values** (for regression).\n",
    "\n",
    "![](https://learning.oreilly.com/library/view/machine-learning-pocket/9781492047537/assets/mlpr_0301.png)\n",
    "\n",
    "This is standard naming procedure for naming the data and the output. If you read academic papers or even look at the documentation for libraries, they follow this convention. In Python, we use the variable name $X$ to hold the sample data even though capitalization of variables is a violation of standard naming conventions (PEP 8). Don’t worry, everyone does it, and if you were to name your variable x, they might look at you funny. The variable $y$ stores the labels or targets.\n",
    "\n",
    "The table shows a basic dataset with two samples and three features for each sample.\n",
    "\n",
    "|pclass|age|sibsp|\n",
    "|------|---|-----|\n",
    "|1|29|0|\n",
    "|1|2|1|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load an **Excel file** (make sure you have `pandas` and `xlrd1` installed) with the Titanic features. It has many columns, including a survived column that contains the label of what happened to an individual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\n",
    "     \"http://biostat.mc.vanderbilt.edu/\"\n",
    "     \"wiki/pub/Main/DataSets/titanic3.xls\"\n",
    ")\n",
    "df = pd.read_excel(url)\n",
    "orig_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following columns are included in the dataset:\n",
    "\n",
    "- `pclass` Passenger class (1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    "- `survival` Survival (0 = No, 1 = Yes)\n",
    "- `name` Name\n",
    "- `sex` Sex\n",
    "- `age` Age\n",
    "- `sibsp` Number of siblings/spouses aboard\n",
    "- `parch` Number of parents/children aboard\n",
    "- `ticket` Ticket number\n",
    "- `fare` Passenger fare\n",
    "- `cabin` Cabin\n",
    "- `embarked` Point of embarkation (`C` = Cherbourg, `Q` = Queenstown, `S` = Southampton)\n",
    "- `boat` Lifeboat\n",
    "- `body` Body identification number\n",
    "- `home.dest` Home/destination\n",
    "\n",
    "Pandas can read this spreadsheet and convert it into a `DataFrame` for us. We will need to spot-check the data and ensure that it is OK for performing analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the data, we need to ensure that it is in a format that we can use to create a model. Most scikit-learn models require that our features be **numeric** (`integer` or `float`). In addition, many models fail if they are passed **missing values** (`NaN` in pandas or numpy). Some models perform better if the data is **standardized** (given a mean value of $0$ and a standard deviation of $1$). We will deal with these issues using pandas or scikit-learn. In addition, the Titanic dataset has leaky features.\n",
    "\n",
    "**Leaky features** are variables that contain information about the future or target. There’s nothing bad in having data about the target, and we often have that data during model creation time. However, if those variables are not available when we perform a prediction on a new sample, we should remove them from the model as they are leaking data from the future.\n",
    "\n",
    "Cleaning the data can take a bit of time. It helps to have access to a subject matter expert (SME) who can provide guidance on dealing with outliers or missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We typically see `int64, float64, datetime64[ns]`, or `object`. These are the types that pandas uses to store a column of data. `int64` and `float64` are **numeric types**. `datetime64[ns]` holds **date and time data**. `object` typically means that it is holding **string data**, though it could be a combination of string and other types.\n",
    "\n",
    "When reading from CSV files, pandas will try to coerce data into the appropriate type, but will fall back to object. Reading data from spreadsheets, databases, or other systems may provide better types in the DataFrame. In any case, it is worthwhile to look through the data and ensure that the types make sense.\n",
    "\n",
    "Integer types are typically fine. Float types might have some missing values. Date and string types will need to be converted or used to feature engineer numeric types. String types that have **low cardinality** are called **categorical columns**, and it might be worthwhile to create **dummy columns** from them (the `pd.get_dummies()` function takes care of this)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pandas-profiling` library includes a **profile report**. You can generate this report in a notebook. It will summarize the types of the columns and allow you to view details of quantile statistics, descriptive statistics, a histogram, common values, and extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "\n",
    "pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `.shape` attribute of the DataFrame to inspect the number of rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `.describe` method to get **summary stats** as well as see the count of **nonnull data**. The default behavior of this method is to only report on numeric columns. Here the output is truncated to only show the first two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count statistic only includes values that are not `NaN`, so it is useful for checking whether a column is missing data. It is also a good idea to spot-check the minimum and maximum values to see if there are **outliers**. Summary statistics are one way to do this. Plotting a histogram or a box plot is a visual representation that we will see later.\n",
    "\n",
    "We will need to deal with missing data. Use the `.isnull` method to find columns or rows with missing values. Calling `.isnull` on a DataFrame returns a new DataFrame with every cell containing a True or False value. In Python, these values evaluate to 1 and 0, respectively. This allows us to sum them up or even calculate the percent missing (by calculating the mean).\n",
    "\n",
    "The code indicates the count of missing data in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Replace `.sum` with `.mean` to get the percentage of null values. By default, calling these methods will apply the operation along `axis 0`, which is along the index. If you want to get the counts of missing features for each sample, you can apply this along `axis 1` (along the columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SME can help in determining what to do with missing data. The `age` column might be useful, so keeping it and interpolating values could provide some signal to the model. Columns where **most of the values are missing** (cabin, boat, and body) tend to not provide value and can be dropped.\n",
    "\n",
    "The `body` column (body identification number) is missing for many rows. We should drop this column at any rate because it **leaks data**. This column indicates that the passenger did not survive; by necessity our model could use that to cheat. We will pull it out. (If we are creating a model to predict if a passenger would die, knowing that they had a body identification number a priori would let us know they were already dead. We want our model to not know that information and make the prediction based on the other columns.) Likewise, the boat column leaks the reverse information (that a passenger survived).\n",
    "\n",
    "Let’s look at some of the rows with missing data. We can create a boolean array (a series with True or False to indicate if the row has missing data) and use it to inspect rows that are missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.isnull().any(axis=1)\n",
    "\n",
    "df[mask].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will impute (or derive values for) the missing values for the age column later.\n",
    "\n",
    "Columns with type of object tend to be categorical (but they may also be high cardinality string data, or a mix of column types). For object columns that we believe to be categorical, use the `.value_counts` method to examine the counts of the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sex.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that pandas typically ignores `null` or `NaN` values. If you want to include those, use `dropna=False` to also show counts for `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.embarked.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a couple of options for dealing with missing embarked values. Using `S` might seem logical as that is the **most common value**. We could dig into the data and try and determine if another option is better. We could also drop those two values. Or, because this is categorical, we can ignore them and use pandas to create **dummy columns** if these two samples will just have 0 entries for every option. We will use this latter choice for this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop columns that have no variance or no signal. There aren’t features like that in this dataset, but if there was a column called “is human” that had 1 for every sample this column would not be providing any information.\n",
    "\n",
    "Alternatively, unless we are using NLP or extracting data out of text columns where **every value is different**, a model will not be able to take advantage of this column. The `name` column is an example of this. Some have pulled out the title t from the name and treated it as categorical.\n",
    "\n",
    "We also want to drop columns that leak information. Both `boat` and `body` columns leak whether a passenger survived.\n",
    "\n",
    "The pandas `.drop` method can drop either rows or columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(\"object\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"name\", \"ticket\", \"home.dest\", \"boat\", \"body\", \"cabin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create **dummy columns** from string columns. This will create new columns for `sex` and `embarked`. Pandas has a convenient `get_dummies` function for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the `sex_male` and `sex_female` columns are perfectly **inverse correlated**. Typically we remove any columns with perfect or very high positive or negative correlation. **Multicollinearity** can impact interpretation of feature importance and coefficients in some models. Here is code to remove the `sex_male` colum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=\"sex_male\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can add a `drop_first=True` parameter to the get_dummies call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `DataFrame (X)` with the features and a `series (y)` with the labels. We could also use numpy arrays, but then we don’t have column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=\"survived\")\n",
    "y = df.survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always want to train and test on different data. Otherwise you don’t really know how well your model **generalizes** to data that it hasn’t seen before. We’ll use scikit-learn to pull out $30$% for testing (using `random_state=42` to remove an element of randomness if we start comparing different models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# here I would split with stratify=survived ...\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `age` column has missing values. We need to **impute** age from the numeric values. We only want to impute on the training set and then use that imputer to fill in the date for the test set. Otherwise we are leaking data (cheating by giving future information to the model).\n",
    "\n",
    "Now that we have test and train data, we can impute missing values on the training set, and use the trained imputers to fill in the test dataset. The fancyimpute library has many algorithms that it implements. Sadly, most of these algorithms are not implemented in an inductive manner. This means that you cannot call `.fit` and then `.transform`, which means you cannot impute for new data based on how the model was trained.\n",
    "\n",
    "The `IterativeImputer` class (which was in fancyimpute but has been migrated to scikit-learn) does support inductive mode. To use it we need to add a special experimental import (as of scikit-learn version 0.21.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "X_train.loc[:,:] = imputer.fit_transform(X_train)\n",
    "X_test.loc[:,:] = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to impute with the median, we can use pandas to do that (or the `SimpleImputer` class of sklearn):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meds = X_train.median()\n",
    "X_train = X_train.fillna(meds)\n",
    "X_test = X_test.fillna(meds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalizing** or preprocessing the data will help many models perform better after this is done. Particularly those that depend on a **distance metric** to **determine similarity**. (Note that tree models, which treat each feature on its own, don’t have this requirement.)\n",
    "\n",
    "We are going to **standardize** the data for the preprocessing. **Standardizing** is translating the data so that it has a **mean value of zero** and a **standard deviation of one**. This way models don’t treat variables with larger scales as more important than smaller scaled variables. I’m going to stick the result (numpy array) back into a pandas DataFrame for easier manipulation (and to keep column names).\n",
    "\n",
    "I also normally don’t standardize dummy columns, so I will ignore those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cols = X_train.columns\n",
    "sca = StandardScaler()\n",
    "\n",
    "X_train = sca.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns=cols)\n",
    "\n",
    "X_test = sca.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.mean().round(), X_train.std().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I like to refactor my code. I typically make two functions. One for **general cleaning**, and another for **dividing up into a training and testing set and to perform mutations** that need to happen differently on those sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def tweak_titanic(df):\n",
    "    df = df.drop(\n",
    "    columns=[\n",
    "        \"name\",\n",
    "        \"ticket\",\n",
    "        \"home.dest\",\n",
    "        \"boat\",\n",
    "        \"body\",\n",
    "        \"cabin\",\n",
    "    ]).pipe(pd.get_dummies, drop_first=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_train_test_X_y(df, y_col, size=0.3, std_cols=None):\n",
    "    y = df[y_col]\n",
    "    X = df.drop(columns=y_col)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=size, random_state=42\n",
    "    )\n",
    "    \n",
    "    cols = X.columns\n",
    "    num_cols = [\n",
    "        \"pclass\",\n",
    "        \"age\",\n",
    "        \"sibsp\",\n",
    "        \"parch\",\n",
    "        \"fare\",\n",
    "    ]\n",
    "    \n",
    "    fi = IterativeImputer()\n",
    "    \n",
    "    X_train.loc[:, num_cols] = fi.fit_transform(X_train[num_cols])\n",
    "    X_test.loc[:, num_cols] = fi.transform(X_test[num_cols])\n",
    "\n",
    "    if std_cols:\n",
    "        std = StandardScaler()\n",
    "        X_train.loc[:, std_cols] = std.fit_transform(X_train[std_cols])\n",
    "        X_test.loc[:, std_cols] = std.transform(X_test[std_cols])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti_df = tweak_titanic(orig_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_cols = \"pclass,age,sibsp,fare\".split(\",\")\n",
    "X_train, X_test, y_train, y_test = get_train_test_X_y(df=ti_df, y_col=\"survived\", std_cols=std_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a **baseline model** that does something really simple can give us something to compare our model to. Note that using the default `.score` result gives us the accuracy which can be misleading. A problem where a positive case is $1$ in $10,000$ can easily get over $99$% accuracy by always predicting negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "bm = DummyClassifier(strategy=\"stratified\")\n",
    "bm.fit(X_train, y_train)\n",
    "bm.score(X_test, y_test)  # accuracy is misleading for imbalanced problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.precision_score(y_test, bm.predict(X_test))  # precision = TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various Families"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code tries a variety of algorithm families. The “No Free Lunch” theorem states that no algorithm performs well on all data. However, for some finite set of data, there may be an algorithm that does well on that set. (A popular choice for structured learning these days is a tree-boosted algorithm such as **XGBoost**.)\n",
    "\n",
    "Here we use a few different families and compare the **AUC score** and standard deviation using k-fold cross-validation. An algorithm that has a slightly smaller average score but tighter standard deviation might be a better choice.\n",
    "\n",
    "Because we are using k-fold cross-validation, we will feed the model all of `X` and `y`.\n",
    "\n",
    "**Me**: I do not support the last point as the test set should be kept secret and hidden especially when testing different model families. Especially because CV tests different portions of the training data it should be no problem to do CV on the training data and keep the test set for the final selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for model in [\n",
    "    DummyClassifier, \n",
    "    LogisticRegression, \n",
    "    DecisionTreeClassifier, \n",
    "    KNeighborsClassifier, \n",
    "    GaussianNB, \n",
    "    SVC,\n",
    "    RandomForestClassifier,\n",
    "    xgboost.XGBClassifier\n",
    "]:\n",
    "    cls = model() # we keep the defaults\n",
    "    kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    s = model_selection.cross_val_score(cls, X_train, y_train, scoring=\"roc_auc\", cv=kfold)\n",
    "    \n",
    "    result.append(pd.DataFrame({'model': [model.__name__], \"AUC\": [s.mean()], \"STD\": [s.std()]}))\n",
    "    print(f\"{model.__name__:22} AUC: {s.mean():.3f} STD: {s.std():.2f}\")\n",
    "    \n",
    "df_result = pd.concat(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.set_index(\"model\").plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were going down the Kaggle route (or want **maximum performance at the cost of interpretability**), stacking is an option. A stacking classifier takes other models and uses their output to predict a target or label. We will use the previous models’ outputs and combine them to see if a stacking classifier can do better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import (StackingClassifier)\n",
    "clfs = [\n",
    "    x() for x in [\n",
    "        LogisticRegression,\n",
    "        DecisionTreeClassifier,\n",
    "        KNeighborsClassifier,\n",
    "        GaussianNB,\n",
    "        SVC,\n",
    "        RandomForestClassifier,\n",
    "    ]\n",
    "]\n",
    "\n",
    "stack = StackingClassifier(classifiers=clfs, meta_classifier=LogisticRegression())\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "s = model_selection.cross_val_score(stack, X_train, y_train, scoring=\"roc_auc\", cv=kfold)\n",
    "\n",
    "print(\n",
    " f\"{stack.__class__.__name__}  \"\n",
    " f\"AUC: {s.mean():.3f}  STD: {s.std():.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case it looks like performance went down a bit, as well as standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’m going to use a random forest classifier to create a model. It is a flexible model that tends to give decent out-of-the-box results. Remember to train it (calling .fit) with the training data from the data that we split earlier into a training and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model, we can use the test data to see how well the model **generalizes** to data that it hasn’t seen before. The `.score` method of a classifier returns the **average of the prediction accuracy**. We want to make sure that we call the `.score` method with the test data (presumably it should perform better with the training data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at other metrics, such as precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_score(y_test, rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice benefit of tree-based models is that you can inspect the feature importance. The **feature importance** tells you how much a **feature contributes to the model**. Note that removing a feature doesn’t mean that the score will go down accordingly, as other features might be colinear (in this case we could remove either the sex_male or sex_female column as they have a perfect negative correlation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, val in sorted(zip(X_train.columns, rf.feature_importances_), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{col:10}{val:10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he feature importance is calculated by looking at the error increase. If removing a feature increases the error in the model, the feature is more important.\n",
    "\n",
    "I really like the **SHAP library** for exploring what features a model deems important, and for explaining predictions. This library works with black-box models, and we will show it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models have **hyperparameters** that control how they behave. By varying the values for these parameters, we change their **performance**. Sklearn has a **grid search** class to evaluate a model with different combinations of parameters and return the best result. We can use those parameters to instantiate the model class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf4 = RandomForestClassifier()\n",
    "params = {\n",
    "     \"max_features\": [0.4, \"auto\"],\n",
    "     \"n_estimators\": [15, 200],\n",
    "     \"min_samples_leaf\": [1, 0.1],\n",
    "     \"random_state\": [42],\n",
    "}\n",
    "cv = GridSearchCV(\n",
    "     rf4, params, n_jobs=-1\n",
    ").fit(X_train, y_train)\n",
    "print(cv.best_params_)\n",
    "print(cv.score(X_test, y_test))\n",
    "\n",
    "rf5 = RandomForestClassifier(\n",
    "     **{\n",
    "         \"max_features\": \"auto\",\n",
    "         \"min_samples_leaf\": 0.1,\n",
    "         \"n_estimators\": 200,\n",
    "         \"random_state\": 42,\n",
    "     }\n",
    ")\n",
    "rf5.fit(X_train, y_train)\n",
    "rf5.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass in a scoring parameter to `GridSearchCV` to optimize for different metrics. See Chapter 12 for a list of metrics and their meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A confusion matrix allows us to see the correct classifications as well as **false positives** and **false negatives**. It may be that we want to optimize toward false positives or false negatives, and different models or parameters can alter that. We can use `sklearn` to get a text version, or `Yellowbrick` for a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = rf5.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "mapping = {0: \"died\", 1: \"survived\"}\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "cm_viz = ConfusionMatrix(\n",
    "    rf5,\n",
    "    classes=[\"died\", \"survived\"],\n",
    "    label_encoder=mapping,\n",
    ")\n",
    "\n",
    "cm_viz.score(X_test, y_test)\n",
    "cm_viz.poof()\n",
    "fig.savefig(\n",
    "    \"images/mlpr_0304.png\",\n",
    "     dpi=300,\n",
    "     bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yellowbrick confusion matrix. This is a useful evaluation tool that presents the predicted class along the bottom and the true class along the side. A good classifier would have all of the values along the diagonal, and zeros in the other cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **receiver operating characteristic (ROC)** plot is a common tool used to evaluate classifiers. By measuring the **area under the curve (AUC)**, we can get a metric to **compare different classifiers**. It plots the **true positive rate** against the **false positive rate**. We can use sklearn to calculate the AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred = rf5.predict(X_test)\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier.rocauc import roc_auc\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "roc_viz = roc_auc(rf5, X_train, y_train)\n",
    "print(roc_viz.score(X_test, y_test))\n",
    "\n",
    "roc_viz.poof()\n",
    "fig.savefig(\"../img/mlpr_0305.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve\n",
    "\n",
    "A learning curve is used to tell us if we have enough training data. It trains the model with increasing portions of the data and measures the score. If the cross-validation score continues to climb, then we might need to **invest in gathering more data**. Here is a Yellowbrick example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "cv = StratifiedKFold(n_splits=12)\n",
    "sizes = np.linspace(0.3, 1.0, 10)\n",
    "lc_viz = LearningCurve(\n",
    "    rf5,\n",
    "    cv=cv,\n",
    "    train_sizes=sizes,\n",
    "    scoring=\"f1_weighted\",\n",
    "    n_jobs=4,\n",
    "    ax=ax,\n",
    ")\n",
    "lc_viz.fit(X_train, y_train)\n",
    "lc_viz.poof()\n",
    "fig.savefig(\"../img/mlpr_0306.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 (ml-basic)",
   "language": "python",
   "name": "ml-basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
